{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install scapy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json,boto3\n",
    "from scapy.all import *\n",
    "import csv,binascii\n",
    "\n",
    "sess = sagemaker.Session(default_bucket='packets-bucket-web')\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'packets-bucket-web'\n",
    "prefix = 'text-classification'\n",
    "training_ground_truth_files = ['UNSW-NB15_1.csv', 'UNSW-NB15_2.csv', 'UNSW-NB15_3.csv']\n",
    "training_pcap_files = ['1.pcap', '2.pcap', '3.pcap']\n",
    "testing_pcap_files = ['4.pcap']\n",
    "testing_ground_truth_files = ['UNSW-NB15_4.csv']\n",
    "# Paketų kiekis kuris bus imamamas iš kiekvieno pcap failo\n",
    "num_packets = 50000 # Pakeisiti jeigu atitinkamai ruosiama mokymui/testavimui\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!unzip training_data/csv_files.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def process_packets(packet_file,csv_file):\n",
    "    new_packets = []\n",
    "    packets = PcapReader(packet_file)\n",
    "    counter0 = 0\n",
    "    counter1 = 0\n",
    "    size = 0\n",
    "    for packet in packets:\n",
    "        if IP in packet:\n",
    "            src_ip = packet[IP].src\n",
    "            dst_ip = packet[IP].dst  \n",
    "        if TCP in packet:\n",
    "            data = packet[TCP].payload\n",
    "            data = binascii.hexlify(bytes(data))\n",
    "            tcp_sport=packet.sport\n",
    "            tcp_dport=packet.dport\n",
    "            packet_time = packet.time\n",
    "            if len(data) != 0:\n",
    "                for row in csv_file:\n",
    "                    if row[0] == src_ip and row[1] == tcp_sport and row[2] == dst_ip and row[3] == tcp_dport and row[49] == packet_time:\n",
    "                        if row[61] == 0:\n",
    "                            if counter0 == num_packets/2:\n",
    "                                packet_class = 1\n",
    "                                counter1 = counter1 + 1\n",
    "                            else:\n",
    "                                counter0 = counter0 + 1\n",
    "                        else:\n",
    "                            if counter1 == num_packets/2:\n",
    "                                packet_class = 0\n",
    "                                counter0 = counter0 + 1\n",
    "                            else:\n",
    "                                counter1 = counter1 + 1\n",
    "                    new_packet = tuple((packet_class, data))\n",
    "                    new_packets.append(new_packet)\n",
    "                    size = size + 1\n",
    "                    print(\"Added new packet\")\n",
    "                    if size > num_packets:\n",
    "                        break\n",
    "            else:\n",
    "                print(\"skipping packet: empty payload\")\n",
    "        else:\n",
    "            print(\"skipping packet: no TCP found\")\n",
    "    print(\"Number of class 1 packets: \",counter0,\"Number of class 2 packets: \",counter1)\n",
    "    return new_packets\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# visi apmokymui skirti failai apdorojami ir sujungiami į naują bendrą masyvą\n",
    "combined_list_new_packets = []\n",
    "for packet_file,csv_file in zip(training_pcap_files, training_ground_truth_files):\n",
    "    #CSV failu nuskaitymas i masyva\n",
    "    with open(csv_file) as f:\n",
    "            file_read = csv.reader(f)\n",
    "    data = process_packets(packet_file,file_read)\n",
    "    combined_list_new_packets.extend(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('training.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for packet in combined_list_new_packets:\n",
    "        writer.writerow(packet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# visi testavimui skirti failai apdorojami ir sujungiami į naują bendrą masyvą\n",
    "combined_list_new_packets = []\n",
    "for packet_file,csv_file in zip(testing_pcap_files, testing_ground_truth_files):\n",
    "    #CSV failu nuskaitymas i masyva\n",
    "    with open(csv_file) as f:\n",
    "        file_read = csv.reader(f)\n",
    "    process_packets(packet_file,csv_file)\n",
    "    combined_list_new_packets.extend(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('testing.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for packet in combined_list_new_packets:\n",
    "        writer.writerow(packet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "index_to_label = {}\n",
    "with open(\"training_data/classes.txt\") as f:\n",
    "    for i, label in enumerate(f.readlines()):\n",
    "        index_to_label[str(i)] = label.strip()\n",
    "print(index_to_label)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  # Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    return cur_row"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, \"r\") as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=\",\")\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[: int(keep * len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    with open(output_file, \"w\") as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=\" \", lineterminator=\"\\n\")\n",
    "        csv_writer.writerows(transformed_rows)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess(\"training.csv\", \"packets.train\", keep=0.2)\n",
    "\n",
    "# Preparing the validation dataset\n",
    "preprocess(\"testing.csv\", \"packets.validation\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "sess.upload_data(path=\"packets.train\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"packets.validation\", bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "region_name = boto3.Session().region_name"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=30,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 1,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"vector_dim\": 10,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 4,\n",
    "        \"min_epochs\": 5,\n",
    "        \"word_ngrams\": 2,\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}\n",
    "\n",
    "\n",
    "access_key = \"\"\n",
    "secret_access_key = \"\""
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
